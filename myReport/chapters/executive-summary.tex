% Executive summary (1 page) — A 1-page summary of the project, to include the main research idea or hypothesis, the main features of the experimental design, a summary of the main result(s), and a conclusion.
\chapter{Executive Summary}
\section{Needle in the Haystack: Investigating Sparse Functional Status Information with NLP}
\subsection{Motivation and Objective}

Functional Status Information (FSI), descriptions of how well patients carry out everyday tasks, remains hidden in the free text of electronic health records. Revealing it can sharpen clinical insight and make care more patient-centred. This project therefore asked whether natural-language processing (NLP) can detect and classify FSI sentences in MIMIC-IV discharge summaries, assigning each sentence to one of four International Classification of Functioning (ICF) domains, which are Mobility, Self-Care and Domestic Life, Interpersonal Interactions and Relationships (IPIR) and Communication and Cognition. \medskip

\subsection{Experimental Design and Approach}

Because no labelled corpus existed, the study pursued two complementary strategies. In the supervised part, the team created a silver-standard dataset by manually annotating sentences according to ICF guidelines and then trained three contrasting models. A convolutional–recurrent network used FastText embeddings plus convolutional layers for local patterns and gated recurrent layers for context. A lighter feed-forward network relied on mean-pooled embeddings and served as a baseline, while a Random Forest combined TF-IDF features with FastText vectors to offer an interpretable alternative. In the unsupervised arm, Latent Dirichlet Allocation extracted topic mixtures from bag-of-words representations, K-Means clustering grouped sentences by contextual embeddings from ClinicalBERT and BioBERT, and a rule-based variant seeded clusters with high-confidence ICF keywords before propagating labels to similar sentences. \medskip

\subsection{Main Results}
Amongst the supervised models, a hybrid CNN-RNN achieved the highest performance with a micro-average F1 score of 0.74 - outperforming the fully connected neural network (micro-F1 0.67) and the Random Forest classifier. Sentences pertaining to mobility were identified accurately with a high precision and recall, which may be attributed to a greater abundance of sentences overall for this category in the discharge summaries. The models performed well on sentences related to Communication and Cognition, despite the comparatively limited number of examples in this category.\medskip

Sentences for the IPIR category proved comparatively challenging. It is important to note that due to nuances in its annotation guidelines \cite{InterpersonalGuideline2023}, these were especially complex for manual annotators to identify too. \medskip

The Unsupervised approach with K-Means semantic clustering followed similar trends: it accurately grouped sentences belonging to categories such as mobility and communication, but struggled to isolate the rest. The K-Means clustering with pre-trained Clinical Bert embeddings showed the best performance amongst unsupervised approaches.\medskip

Overall, these findings highlight both the promise and limitations of using NLP techniques for FSI extraction. While high performance was achieved in categories with clearer lexical patterns and more training data, performance can vary substantially across functional domains. This highlights the need for category-specific modelling strategies and improved annotation resources to support more accurate, generalisable and increasingly more nuanced classification.

\subsection{Conclusion and Challenges}
One of the most critical issues was the quality and consistency of manual annotations, particularly across the four ICF categories. Given that annotations were performed by non-experts, variation in interpretation led to label noise that affected the reliability of supervised learning models. Additionally, there was a pronounced class imbalance in the dataset, with Mobility-related sentences overrepresented compared to under-represented domains such as Communication and Cognition. This skew introduced performance bias and limited the models’ ability to generalise across all functional domains. A third challenge lay in the inherent ambiguity of language used to describe functional status; relevant information was often context-dependent or expressed in subtle, implicit ways that were difficult for both human annotators and models to capture.

These limitations underscore the need for a more robust and scalable foundation for future work. In particular, the development of a gold-standard dataset, constructed in collaboration with domain experts, should be prioritised. Such a resource would significantly reduce non-expert label ambiguity and allow for more accurate training, evaluation, and benchmarking of FSI extraction models. \medskip

To address data imbalance and linguistic ambiguity, future work may focus on the exploration of strategies such as data augmentation, semi-supervised learning, and the development of domain-adapted language models suitable for medical use. Techniques like active learning could also improve annotation efficiency and are addressed in greater detail in the discussion and conclusions chapter.\medskip

Despite these challenges, this project demonstrates the feasibility of extracting sparse but clinically valuable FSI from unstructured EHR text. It provides a proof of concept for integrating functional information into computational pipelines and lays the groundwork for more comprehensive, patient-centred data extraction efforts within electronic health record systems.